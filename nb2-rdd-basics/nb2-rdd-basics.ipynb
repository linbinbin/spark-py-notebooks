{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDDの基本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Introduction to Spark with Python, by Jose A. Dianes](https://github.com/jadianes/spark-py-notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このノートブックでは、3つの基本的なスパーク操作について説明します。そのうち2つは変換マップとフィルタです。もう1つはアクション収集です。同時に、Sparkの永続性の概念を紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの取得とRDDの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初のノートブックで行ったように、KDD Cup 1999に提供された縮小データセット（10％）を使用し、約50万のネットワークインタラクションが含まれます。このファイルは、ローカルでダウンロードするGzipファイルとして提供されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "#\"\"\"print(dir(urllib.request))\"\"\"\n",
    "f = urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでRDDを作成することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## フィルタ変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この変換は、特定の条件を満たす要素だけを保持するために、RDDに適用できます。 より具体的には、元のRDDのすべての要素について関数を評価する。 新しい結果のRDDには、関数をTrueに戻す要素だけが含まれます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "たとえば、正常な数を数えたいとします。私たちがデータセットに持つ相互作用。 raw_data RDDを次のようにフィルタリングできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,219,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,39,39,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,217,2032,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,49,49,1.00,0.00,0.02,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.count()\n",
    "normal_raw_data = raw_data.filter(lambda x: 'normal.' in x)\n",
    "#raw_data.take(5)\n",
    "normal_raw_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 97278 'normal' interactions\n",
      "Count completed in 6.046 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "normal_count = normal_raw_data.count()\n",
    "tt = time() - t0\n",
    "print(\"There are {} 'normal' interactions\".format(normal_count))\n",
    "print(\"Count completed in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、新しいRDDにいくつの要素があるのか​​を数えることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ノートブック1から、10％のデータセットに合計494021があることを忘れないでください。 ここでは、97278に正常が含まれていることがわかります。\n",
    "タグワード。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDの要素を数えるための経過時間を測定したことに注目してください。 Sparkの実際の（分散した）計算は、アクションではなく変換を実行するときに行われることを指摘したいので、これを行っています。 この場合、カウントはRDDで実行するアクションです。 私たちは、RDD上で必要な数の変換を適用することができます。この場合、完了するのに数秒かかる最初のアクションを呼び出すまで計算は行われません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## マップ変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparkのマップ変換を使用することで、RDDのすべての要素に関数を適用できます。 Pythonのラムダは、特にこのような表現に特化しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この場合、私たちはデータファイルをCSV形式のファイルとして読みたいと思っています。 RDDの各要素にラムダ関数を適用すると、次のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse completed in 2.096 seconds\n",
      "['0',\n",
      " 'tcp',\n",
      " 'http',\n",
      " 'SF',\n",
      " '181',\n",
      " '5450',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '1',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '8',\n",
      " '8',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '1.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '9',\n",
      " '9',\n",
      " '1.00',\n",
      " '0.00',\n",
      " '0.11',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " 'normal.']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "t0 = time()\n",
    "head_rows = csv_data.take(5)\n",
    "tt = time() - t0\n",
    "print(\"Parse completed in {} seconds\".format(round(tt,3)))\n",
    "pprint(head_rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この場合も、最初のSparkアクション（この場合はtake）を呼び出すと、すべてのアクションが発生します。最初の数人だけでなく、たくさんの要素を取るとどうなりますか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse completed in 6.422 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "head_rows = csv_data.take(100000)\n",
    "tt = time() - t0\n",
    "print( \"Parse completed in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "時間がかかることがわかります。マップ関数は、RDD上の多くの要素に分散して適用されるため、実行時間が長くなります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### マップと定義済みの関数の使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もちろん、あらかじめ定義された関数をmapで使うことができます。 RDD内の各要素を、キーがタグ（たとえばノーマル）であり、値がCSV形式のファイルの行を表す要素のリスト全体であるキーと値のペアとして使用するとします。 我々は次のように進めることができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('normal.',\n",
      " ['0',\n",
      "  'tcp',\n",
      "  'http',\n",
      "  'SF',\n",
      "  '181',\n",
      "  '5450',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '1',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '8',\n",
      "  '8',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '1.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '9',\n",
      "  '9',\n",
      "  '1.00',\n",
      "  '0.00',\n",
      "  '0.11',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  'normal.'])\n"
     ]
    }
   ],
   "source": [
    "def parse_interaction(line):\n",
    "    elems = line.split(\",\")\n",
    "    tag = elems[41]\n",
    "    return (tag, elems)\n",
    "\n",
    "key_csv_data = raw_data.map(parse_interaction)\n",
    "head_rows = key_csv_data.take(5)\n",
    "pprint(head_rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "簡単でしたね。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "キーと値のペアを扱うノートブックでは、このタイプのRDDを使用してデータ集約（たとえば、キーでカウント）を行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collectアクション"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまでのところ、conutとtakeを使用しています。 私たちが学ぶ必要があるもう一つの基本的な行動は、mapです。 基本的には、RDD内のすべての要素をメモリに入れて、それらの要素と一緒に作業します。 このため、特に大きなRDDで作業する場合は、注意して使用する必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生データを使用した例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected in 8.981 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "all_raw_data = raw_data.collect()\n",
    "tt = time() - t0\n",
    "print(\"Data collected in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それは当然以前に使用した他の行動と同じくらい時間がかかりました。 RDDのフラグメントを持つすべてのSparkワーカーノードは、その部分を取り出し、すべてを一緒に減らすために調整する必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまでのすべてを組み合わせた最後の例として、すべての正常なやりとりをキーと値のペアとして収集したいと考えています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected in 10.086 seconds\n",
      "There are 97278 'normal' interactions\n"
     ]
    }
   ],
   "source": [
    "# get data from file\n",
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "\n",
    "# parse into key-value pairs\n",
    "key_csv_data = raw_data.map(parse_interaction)\n",
    "\n",
    "# filter normal key interactions\n",
    "normal_key_interactions = key_csv_data.filter(lambda x: x[0] == \"normal.\")\n",
    "\n",
    "# collect all\n",
    "t0 = time()\n",
    "all_normal = normal_key_interactions.collect()\n",
    "tt = time() - t0\n",
    "normal_count = len(all_normal)\n",
    "print(\"Data collected in {} seconds\".format(round(tt,3)))\n",
    "print(\"There are {} 'normal' interactions\".format(normal_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このカウントは、通常のインタラクションの前のカウントと一致します。 新しい手順はより時間がかかります。 これは、collectですべてのデータを取得し、結果リストにPythonのlenを使用するためです。 以前は、カウントを使用してRDD内の要素の総数を数えていました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
